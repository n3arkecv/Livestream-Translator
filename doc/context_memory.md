# ğŸ§  Context Memory Design â€” Real-Time YouTube Live Translation App

å°æ‡‰æ–‡ä»¶ï¼š

* `/translation_system.md`
* `/dialogue_logging.md`
* `/02_modules_specification.md`
* `/system_logging.md`

---

## ğŸ§­ 1. ç³»çµ±å®šä½

**Context Memory æ¨¡çµ„**ï¼ˆåˆç¨± *Scenario Context Manager*ï¼‰
æ˜¯ç¿»è­¯ç³»çµ±ä¸­ç¶­æŒã€ŒçŸ­æœŸèªå¢ƒé€£çºŒæ€§ã€çš„æ ¸å¿ƒã€‚

å®ƒåœ¨æ¯ä¸€æ¬¡ç¿»è­¯å®Œæˆå¾Œï¼š

* æ“·å–æ–°å¥å…§å®¹èˆ‡ LLM1 ç¿»è­¯çµæœ
* é€é LLM2 (Context Summarizer) æ›´æ–°æƒ…å¢ƒæ‘˜è¦
* ä¿ç•™æœ€è¿‘ 200â€“500 tokens çš„èªå¢ƒè¨˜æ†¶

å…¶è¨­è¨ˆä½¿ LLM ç¿»è­¯æ¯ä¸€å¥æ™‚ï¼Œéƒ½èƒ½çµåˆéå»å¹¾å¥çš„èªå¢ƒè€Œéå–®å¥ç¿»è­¯ã€‚

---

## ğŸ§± 2. æ¶æ§‹ç¸½è¦½

```text
[Fully Transcribed Sentence] + [Scenario Context]
   â”‚
   â–¼
[LLM1 Translator] â”€â–º Translated Sentence
   â”‚
   â–¼
[Display Translated Sentence]
   â”‚
   â–¼
[åŸå§‹ Scenario Context] + [Translated Sentence]
   â”‚
   â–¼
[LLM2 ContextUpdater]
   â”‚
   â”œâ”€â”€â–º å£“ç¸®éå»è¨˜æ†¶
   â”œâ”€â”€â–º æ›´æ–°æƒ…å¢ƒæ‘˜è¦
   â””â”€â”€â–º å»£æ’­ (llm2.context_update_finished) â†’ New Scenario Context
```

---

## ğŸ§© 3. æ¨¡çµ„çµ„æˆ

| æ¨¡çµ„åç¨±                    | è·è²¬                    | é—œéµäº‹ä»¶                                                           |
| ----------------------- | --------------------- | -------------------------------------------------------------- |
| **ContextManager**      | å„²å­˜èˆ‡ç¶­è­·èªå¢ƒè¨˜æ†¶ï¼ˆçŸ­ç¯‡ï¼‰         | `llm2.context_update_started` / `llm2.context_update_finished` |
| **LLM2 ContextUpdater** | å°‡ç•¶å‰ç¿»è­¯çµæœ + éå»æ‘˜è¦ â†’ å£“ç¸®æ›´æ–° | -                                                              |
| **DialogueRecorder**    | æ¥æ”¶æ›´æ–°å¾Œçš„ context ä¸¦å¯«å…¥ç´€éŒ„  | `dialogue.record_appended`                                     |
| **SystemLogger**        | è¨˜éŒ„æ›´æ–°è€—æ™‚èˆ‡è¨˜æ†¶é•·åº¦           | `syslog.info / warning`                                        |

---

## ğŸ“œ 4. è¨˜æ†¶æ¨¡å‹æ¦‚å¿µ

* è¨˜æ†¶å„²å­˜çµæ§‹ï¼š

  ```python
  {
    "sentences": [ { "id": 24, "src": "...", "tr": "..." }, ... ],
    "summary": "..."  # Scenario Context
  }
  ```
* æ¯æ¬¡ç¿»è­¯å®Œæˆå¾Œï¼š

  * LLM1 ä½¿ç”¨ **Scenario Context** + **Fully Transcribed Sentence** é€²è¡Œç¿»è­¯
  * ç¿»è­¯å®Œæˆå¾Œï¼ŒLLM2 ä½¿ç”¨ **åŸå§‹ Scenario Context** + **Translated Sentence** æ›´æ–°æƒ…å¢ƒ
  * æ–°å¢ä¸€å¥åˆ° `sentences`
  * è‹¥ç¸½ token æ•¸è¶…éä¸Šé™ (max_tokens)ï¼Œå‘¼å« LLM2 é€²è¡Œæ‘˜è¦å£“ç¸®
* `summary`ï¼ˆScenario Contextï¼‰æ°¸é ç¶­æŒåœ¨ **200â€“500 tokens** ç¯„åœå…§

---

## âš™ï¸ 5. ä¸»è¦é¡åˆ¥è¨­è¨ˆ

```python
# context/context_manager.py
import tiktoken
import time

class ContextManager:
    def __init__(self, bus, config):
        self.bus = bus
        self.encoder = tiktoken.get_encoding("cl100k_base")
        self.sentences = []
        self.summary = ""
        self.max_tokens = config["context_memory"]["max_tokens"]
        self.model_summary = config["context_memory"]["llm2_model"]

    def append(self, src_text, translated_text, sentence_id):
        """æ–°å¢ä¸€ç­†èªå¥ä¸¦æª¢æŸ¥æ˜¯å¦éœ€è¦å£“ç¸®"""
        self.sentences.append({"id": sentence_id, "src": src_text, "tr": translated_text})
        if self._estimate_tokens() > self.max_tokens:
            self._update_context()

    def _estimate_tokens(self):
        joined = " ".join([s["src"] + " " + s["tr"] for s in self.sentences]) + self.summary
        return len(self.encoder.encode(joined))

    def _update_context(self, original_context: str, translated_sentence: str):
        """ä½¿ç”¨ LLM2 ç”¢ç”Ÿå£“ç¸®æ‘˜è¦
        
        è¼¸å…¥ï¼šåŸå§‹ Scenario Context + Translated Sentence
        è¼¸å‡ºï¼šNew Scenario Context
        """
        start = time.time()
        self.bus.emit(EventName.LLM2_CONTEXT_UPDATE_STARTED, {"context_len_before": self._estimate_tokens()})

        # Prompt çµæ§‹ï¼šä½¿ç”¨åŸå§‹ Scenario Context + Translated Sentence
        prompt = f"""
        è«‹æ›´æ–°æƒ…å¢ƒä¸Šä¸‹æ–‡ï¼Œæ•´åˆæ–°çš„ç¿»è­¯å¥å­ã€‚

        åŸå§‹æƒ…å¢ƒä¸Šä¸‹æ–‡ï¼š
        {original_context}

        æ–°å¢ç¿»è­¯å¥å­ï¼š
        {translated_sentence}

        è«‹ä»¥ç¹é«”ä¸­æ–‡ç”Ÿæˆä¸€æ®µç°¡çŸ­æ‘˜è¦ï¼Œæè¿°ç›®å‰å°è©±çš„æƒ…å¢ƒæˆ–ä¸»é¡Œï¼ˆ200â€“500 tokens å…§ï¼‰ã€‚
        """
        try:
            summary = self._call_llm(prompt)
            self.summary = summary
            self.sentences = self.sentences[-5:]  # ä¿ç•™æœ€è¿‘å¹¾å¥åŸæ–‡
            latency = int((time.time() - start) * 1000)
            self.bus.emit(EventName.LLM2_CONTEXT_UPDATE_FINISHED, {
                "context_len_after": self._estimate_tokens(),
                "context_snippet": summary,
                "latency_ms": latency
            })
        except Exception as e:
            self.bus.emit(EventName.SYSLOG_ERROR, {
                "message": "LLM2 context update failed",
                "exc": str(e)
            })

    def _get_recent_sentences(self, n):
        return "\n".join([f"{s['src']} â†’ {s['tr']}" for s in self.sentences[-n:]])

    def _call_llm(self, prompt):
        from openai import OpenAI
        client = OpenAI()
        response = client.chat.completions.create(
            model=self.model_summary,
            messages=[{"role": "system", "content": "ä½ æ˜¯æƒ…å¢ƒæ‘˜è¦åŠ©ç†ã€‚"},
                      {"role": "user", "content": prompt}]
        )
        return response.choices[0].message.content.strip()
```

---

## ğŸ§  6. Context Update Prompt ç¯„ä¾‹

> ç³»çµ±æç¤ºï¼š
> ã€Œä»¥ä¸‹æ˜¯è¿‘æœŸçš„å°è©±å…§å®¹ï¼Œè«‹ç¶­æŒèªæ°£èˆ‡ä¸»é¡Œä¸€è‡´ï¼Œç”¢ç”Ÿç°¡çŸ­æ‘˜è¦ã€

```
ç›®å‰ä¸»é¡Œï¼šé–‹ç™¼è€…ä»‹ç´¹æ–°åŠŸèƒ½ï¼Œæ­£åœ¨èªªæ˜ç”¨æˆ¶å¦‚ä½•è¨­å®šåƒæ•¸ã€‚
æ–°è¼¸å…¥ï¼š
"This setting helps you control the latency of the response."
â†’ ã€Œæ­¤è¨­å®šå¯è®“æ‚¨æ§åˆ¶å›æ‡‰å»¶é²ã€‚ã€
```

LLM2 å›è¦†ï¼š

> ã€Œè¬›è€…æŒçºŒåœ¨è§£èªªæ‡‰ç”¨ç¨‹å¼æ•ˆèƒ½ç›¸é—œè¨­å®šï¼Œè‘—é‡æ–¼å»¶é²èˆ‡å›æ‡‰é€Ÿåº¦èª¿æ•´ã€‚ã€

---

## ğŸ” 7. è¨˜æ†¶æ›´æ–°ç­–ç•¥

| æ¢ä»¶                | è¡Œç‚º            |
| ----------------- | ------------- |
| ç¸½ token < 200     | ä¸æ›´æ–°           |
| 200 â‰¤ token â‰¤ 500 | è¦–ç‚ºå¥åº·ç‹€æ…‹        |
| token > 500       | å‘¼å« LLM2 ç”¢ç”Ÿæ–°æ‘˜è¦ |
| LLM2 å¤±æ•—           | ä¿ç•™åŸæ‘˜è¦ä¸¦ç´€éŒ„éŒ¯èª¤    |

---

## ğŸ§® 8. Token é ç®—è¦åŠƒ

| æ¨¡å‹                 | Token ä¸Šé™ | Context ä¿ç•™ç¯„åœ      |
| ------------------ | -------- | ----------------- |
| `gpt-4.1-mini`     | 8k       | ä¿ç•™ç´„ 500 tokens    |
| `gpt-4.1`          | 128k     | å¯æ“´è‡³ 1kâ€“2k tokens  |
| `gemini-2.5-flash` | 32k      | ç´„ 500 tokens ç‚ºç†æƒ³å€¼ |

---

## ğŸ§© 9. äº‹ä»¶èˆ‡å›å ±æ ¼å¼

### äº‹ä»¶ï¼š`llm2.context_update_started`

```json
{
  "context_len_before": 612
}
```

### äº‹ä»¶ï¼š`llm2.context_update_finished`

```json
{
  "context_len_after": 285,
  "context_snippet": "è¬›è€…æŒçºŒèªªæ˜å»¶é²è¨­å®šèˆ‡ç³»çµ±å„ªåŒ–ã€‚",
  "latency_ms": 420
}
```

---

## ğŸ§¾ 10. èˆ‡ Dialogue Log çš„æ•´åˆ

1. `llm2.context_update_finished` â†’ ç”± DialogueRecorder æ¥æ”¶
2. æ›´æ–°æœ€å¾Œä¸€ç­†ç´€éŒ„çš„ `"scenario_context"` æ¬„ä½
3. åŒæ­¥å¯«å…¥ `dialogue.jsonl`

æ­¤æ­¥é©Ÿç¢ºä¿èªå¢ƒè¨˜æ†¶èˆ‡èªæ–™ç´€éŒ„ä¸€è‡´ã€‚

---

## ğŸ“Š 11. æ•ˆèƒ½èˆ‡ç›£æ¸¬

| æŒ‡æ¨™                          | èªªæ˜          | ä¾†æº               |
| --------------------------- | ----------- | ---------------- |
| `context_len_before`        | æ›´æ–°å‰ token æ•¸ | ContextManager   |
| `context_len_after`         | æ›´æ–°å¾Œ token æ•¸ | LLM2 å›å‚³          |
| `context_update_latency_ms` | LLM2 å‘¼å«è€—æ™‚   | SystemLog        |
| `update_count`              | ç´¯ç©æ‘˜è¦æ¬¡æ•¸      | MetricsCollector |

---

## ğŸ§© 12. System Log ç¯„ä¾‹

| ç­‰ç´š      | æ¨¡çµ„   | è¨Šæ¯                                                        |
| ------- | ---- | --------------------------------------------------------- |
| INFO    | llm2 | `[Context] update started (tokens=612)`                   |
| INFO    | llm2 | `[Context] summary generated (tokens=285, latency=420ms)` |
| WARNING | llm2 | `[Context] skipped update (tokens<200)`                   |
| ERROR   | llm2 | `[Context] failed to update: APIError 503`                |

---

## ğŸ§° 13. config.json å°æ‡‰è¨­å®š

```json
{
  "context_memory": {
    "enabled": true,
    "max_tokens": 500,
    "llm2_model": "gpt-4.1-mini-2025-04-14"
  }
}
```

---

## ğŸ§® 14. æ¸¬è©¦é …ç›®

* [ ] Context æ›´æ–°äº‹ä»¶æ­£ç¢ºè§¸ç™¼
* [ ] Token è¨ˆç®—èˆ‡é æœŸä¸€è‡´
* [ ] LLM2 å›å‚³æ‘˜è¦å¯è¢«å„²å­˜èˆ‡è¦†å¯«
* [ ] è¶…å‡ºä¸Šé™è‡ªå‹•å£“ç¸®
* [ ] æ›´æ–°å»¶é² < 1 ç§’
* [ ] å°è©±ç´€éŒ„èˆ‡ Context åŒæ­¥
* [ ] ç•°å¸¸æƒ…æ³èƒ½è‡ªå‹•æ¢å¾©å‰ä¸€æ‘˜è¦

---

## ğŸ§± 15. è¨­è¨ˆç†å¿µæ‘˜è¦

* **çŸ­æœŸè¨˜æ†¶å°å‘**ï¼šä¿ç•™æœ€è¿‘èªå¢ƒç‰‡æ®µï¼Œè€Œéæ•´å ´å°è©±ã€‚
* **èªæ„æ‘˜è¦è€Œéæ‹¼æ¥**ï¼šLLM2 ç”¢ç”Ÿèªå¢ƒæ‘˜è¦ï¼Œè€Œéä¸²æ¥æ–‡å­—ã€‚
* **è‡ªå‹•å£“ç¸®**ï¼šç•¶è¨˜æ†¶è¶…æ¨™æ™‚è‡ªå‹•æ›´æ–°ï¼Œé¿å…è¨˜æ†¶çˆ†ç‚¸ã€‚
* **ä¸Šä¸‹æ–‡ä¸€è‡´æ€§**ï¼šç¿»è­¯æ¨¡çµ„æ¯æ¬¡å‘¼å« LLM1 å‰éƒ½é™„å¸¶æœ€æ–°æ‘˜è¦ã€‚
* **æ¨¡çµ„ç¨ç«‹å¯æ›¿æ›**ï¼šContext ç®¡ç†å¯æ›´æ› LLM2 æ¨¡å‹æˆ–ç®—æ³•ã€‚

---

## ğŸ§© 16. ç¸½çµæµç¨‹åœ–

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       ContextManager         â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ Append(sentence, trans) â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚              â–¼               â”‚
â”‚   [Token > 500?]â”€â”€â”€Yesâ”€â”€â”€â”€â”€â”€â–º LLM2 Summarizer
â”‚        â”‚                     â”‚
â”‚       No                     â”‚
â”‚        â–¼                     â”‚
â”‚   ä¿ç•™åŸæ‘˜è¦                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â–¼
   EventBus.emit(llm2.context_update_finished)
            â”‚
            â–¼
     DialogueRecorder æ›´æ–°è¨˜éŒ„
```
